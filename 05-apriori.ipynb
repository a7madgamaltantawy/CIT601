{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# M2E05: Python Implementations of Association Rule Mining\n",
    "\n",
    "# Data Science Master 2021/22\n",
    "\n",
    "# Prof. Dr. Gottfried Vossen\n",
    "\n",
    "In this notebook we look at several Python implementations of association rule mining. We first look at two Apriori implementations and after that into frequent pattern growth.\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. [Association Rule Mining using apyori](#apyori)\n",
    "2. [Using the Apriori implementation of mlxtend](#mlxtend)\n",
    "3. [Shortcomings of Apriori](#shortc)\n",
    "4. [Frequent Pattern Growth](#fpg)\n",
    "5. [An Alternative FP-Growth implementation](#fpg2)\n",
    "6. [Exercise: The LastFM Dataset](#lastfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='img/associationrule.png' alt='Association rule mining' style=\"height:250px;\"/>\n",
    "    <span style='display:block;'>Association rule mining. Image Source: <a href=\"https://theneuralnetworkblog.wordpress.com/2020/05/06/association-rule-mining-apriori/\" target=\"_blank\">The Neural Network Blog</a>.</span>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Association Rule MIning using apyori <a class=\"anchor\" id=\"apyori\"/>\n",
    "\n",
    "Our first Python implementation is described in detail in \"Association Rule Mining via Apriori Algorithm in Python\" by Usman Malik (see https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will apply the Apriori algorithm to a dataset of 7,500 transactions which have been recorded over the course of a week with a French retailer. To this end, the data is loaded into a pandas DataFrame which presents the data in tabular format and which can be derived from a csv or xsl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "store_data = pd.read_csv(os.path.join('Data-20220222', 'store_data.csv'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shrimp</td>\n",
       "      <td>almonds</td>\n",
       "      <td>avocado</td>\n",
       "      <td>vegetables mix</td>\n",
       "      <td>green grapes</td>\n",
       "      <td>whole weat flour</td>\n",
       "      <td>yams</td>\n",
       "      <td>cottage cheese</td>\n",
       "      <td>energy drink</td>\n",
       "      <td>tomato juice</td>\n",
       "      <td>low fat yogurt</td>\n",
       "      <td>green tea</td>\n",
       "      <td>honey</td>\n",
       "      <td>salad</td>\n",
       "      <td>mineral water</td>\n",
       "      <td>salmon</td>\n",
       "      <td>antioxydant juice</td>\n",
       "      <td>frozen smoothie</td>\n",
       "      <td>spinach</td>\n",
       "      <td>olive oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burgers</td>\n",
       "      <td>meatballs</td>\n",
       "      <td>eggs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chutney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>turkey</td>\n",
       "      <td>avocado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mineral water</td>\n",
       "      <td>milk</td>\n",
       "      <td>energy bar</td>\n",
       "      <td>whole wheat rice</td>\n",
       "      <td>green tea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1           2                 3             4   \\\n",
       "0         shrimp    almonds     avocado    vegetables mix  green grapes   \n",
       "1        burgers  meatballs        eggs               NaN           NaN   \n",
       "2        chutney        NaN         NaN               NaN           NaN   \n",
       "3         turkey    avocado         NaN               NaN           NaN   \n",
       "4  mineral water       milk  energy bar  whole wheat rice     green tea   \n",
       "\n",
       "                 5     6               7             8             9   \\\n",
       "0  whole weat flour  yams  cottage cheese  energy drink  tomato juice   \n",
       "1               NaN   NaN             NaN           NaN           NaN   \n",
       "2               NaN   NaN             NaN           NaN           NaN   \n",
       "3               NaN   NaN             NaN           NaN           NaN   \n",
       "4               NaN   NaN             NaN           NaN           NaN   \n",
       "\n",
       "               10         11     12     13             14      15  \\\n",
       "0  low fat yogurt  green tea  honey  salad  mineral water  salmon   \n",
       "1             NaN        NaN    NaN    NaN            NaN     NaN   \n",
       "2             NaN        NaN    NaN    NaN            NaN     NaN   \n",
       "3             NaN        NaN    NaN    NaN            NaN     NaN   \n",
       "4             NaN        NaN    NaN    NaN            NaN     NaN   \n",
       "\n",
       "                  16               17       18         19  \n",
       "0  antioxydant juice  frozen smoothie  spinach  olive oil  \n",
       "1                NaN              NaN      NaN        NaN  \n",
       "2                NaN              NaN      NaN        NaN  \n",
       "3                NaN              NaN      NaN        NaN  \n",
       "4                NaN              NaN      NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains no column headers. In order to avoid that the first row is taken as header, we set \"header=None\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7501 entries, 0 to 7500\n",
      "Data columns (total 20 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       7501 non-null   object\n",
      " 1   1       5747 non-null   object\n",
      " 2   2       4389 non-null   object\n",
      " 3   3       3345 non-null   object\n",
      " 4   4       2529 non-null   object\n",
      " 5   5       1864 non-null   object\n",
      " 6   6       1369 non-null   object\n",
      " 7   7       981 non-null    object\n",
      " 8   8       654 non-null    object\n",
      " 9   9       395 non-null    object\n",
      " 10  10      256 non-null    object\n",
      " 11  11      154 non-null    object\n",
      " 12  12      87 non-null     object\n",
      " 13  13      47 non-null     object\n",
      " 14  14      25 non-null     object\n",
      " 15  15      8 non-null      object\n",
      " 16  16      4 non-null      object\n",
      " 17  17      4 non-null      object\n",
      " 18  18      3 non-null      object\n",
      " 19  19      1 non-null      object\n",
      "dtypes: object(20)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "store_data.info() # Show the number of rows and columns. Check memory usage as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Apriori algorithm\n",
    "\n",
    "[Apriori](https://en.wikipedia.org/wiki/Apriori_algorithm) identifies the frequent individual items in the dataset and extends them to larger and larger item sets as long as those item sets appear sufficiently often in the dataset. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the dataset, as in the following example:\n",
    "\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='img/exampleapriori.png' alt='Example of the Apriori algorithm' style=\"height:500px;\"/>\n",
    "    <span style='display:block;'>Example association rule mining using the Apriori algorithm. Image Source: <a href=\"https://developpaper.com/association-rule-mining-and-apriori-algorithm/\" target=\"_blank\">Developer Paper</a>.</span>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "We will use the [apyori](https://github.com/ymoch/apyori) library developed by [Yu Mochizuki](https://github.com/ymoch/). Apyori offers a simple Python implementation of the Apriori algorithm and uses three measures:\n",
    "\n",
    "1.    Support(B) = (Transactions containing (B))/(Total Transactions)\n",
    "\n",
    "2.    Confidence(A→B) = (Transactions containing both (A and B))/(Transactions containing A)\n",
    "\n",
    "3.    Lift(A→B) = (Confidence (A→B))/(Support (B))\n",
    "\n",
    "where A and B are individual items.\n",
    "\n",
    "To the be able to use the library and run the following code blocks, you first need to install apyori. Open your system's Terminial/Command Prompt and type:\n",
    "\n",
    "```bash\n",
    "pip install apyori\n",
    "```\n",
    "\n",
    "You will need to restart the Jupyter Notebook Kernel to apply the changes. You can do that by clicking on the Jupyter menu Kernel > Restart.\n",
    "\n",
    "<div style=\"align: left; text-align:center;\">\n",
    "    <img src=\"img/restartkernel.png\" alt=\"Restarting Notebook's Kernel\" />\n",
    "    <span style=\"display:block;\">How to restart the Notebook's Kernel.</span>\n",
    "    <br/>\n",
    "</div>\n",
    "\n",
    "After that, you should be able to use apyori by importing it. Eventually the environment in your Anaconda Navigator needs to be switched to \"Anaconda 3\".\n",
    "\n",
    "You are now ready to execute the next code block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm executes the following steps:\n",
    "\n",
    "1. Set lower bounds for support and confidence.\n",
    "\n",
    "2. Extract all item sets having support greater than the min-support.\n",
    "\n",
    "3. Select all rules having confidence greater than the min-confidence.\n",
    "\n",
    "(4. Order the rules according to descending lift.)\n",
    "\n",
    "The Apriori library used here expects a dataset in the form of a list of lists, where the entire dataset is a big (\"outer\") list and each transaction within it is an \"inner\" list. Since the dataset is currently a pandas DataFrame, we convert it into the desired form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apyori\n",
      "  Downloading apyori-1.1.2.tar.gz (8.6 kB)\n",
      "Building wheels for collected packages: apyori\n",
      "  Building wheel for apyori (setup.py): started\n",
      "  Building wheel for apyori (setup.py): finished with status 'done'\n",
      "  Created wheel for apyori: filename=apyori-1.1.2-py3-none-any.whl size=5974 sha256=2279e0cd2331cd795c4a3365f5175aed062f7c0a70cde797f8ee933c0b1002fd\n",
      "  Stored in directory: /Users/ahmedtantawy/Library/Caches/pip/wheels/32/2a/54/10c595515f385f3726642b10c60bf788029e8f3a1323e3913a\n",
      "Successfully built apyori\n",
      "Installing collected packages: apyori\n",
      "Successfully installed apyori-1.1.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "pip.main(['install','apyori'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "    \n",
    "transactions = []\n",
    "\n",
    "# Convert items in the dataframe to string\n",
    "for row in store_data.values: \n",
    "    transactions.append(str(item) for item in row if str(item) != 'nan') # Remove missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply the Apriori algorithm to our dataset; to this end, we use the apriori class from the imported apyori library.\n",
    "\n",
    "The apriori class expects varoius parameters in the following order:\n",
    "\n",
    "1.    The list of lists from wich the rules are to be extracted.\n",
    "\n",
    "2.    The min-support.\n",
    "\n",
    "3.    The min-confidence.\n",
    "\n",
    "4.    The min-lift.\n",
    "\n",
    "5.    The minimum rule length as the minimum number of items in the rules that are generated.\n",
    "\n",
    "Suppose we are interested in rules for only those items that are purchased at least 5 times a day, or 7 x 5 = 35 times in one week, since our dataset is for a one-week time period. The support for those items can be calculated as 35/7500 = 0.0045. The minimum confidence for the rules is 20% or 0.2. Similarly, we specify the value for lift as 3 and finally min_length is 2 since we want at least two products in our rules. These values are arbitrarily chosen and can be changed, so that you can see what difference it makes in the rules you get back out and get a feeling for which values yield interesting results and which do not..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Apriori on top of the transactions\n",
    "results = apriori(transactions, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)\n",
    "\n",
    "association_rules = list(results) # Store rules in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: chicken -> light cream\n",
      "Support: 0.004532728969470737\n",
      "Confidence: 0.29059829059829057\n",
      "Lift: 4.84395061728395\n",
      "---\n",
      "Rule: mushroom cream sauce -> escalope\n",
      "Support: 0.005732568990801226\n",
      "Confidence: 0.3006993006993007\n",
      "Lift: 3.790832696715049\n",
      "---\n",
      "Rule: escalope -> pasta\n",
      "Support: 0.005865884548726837\n",
      "Confidence: 0.3728813559322034\n",
      "Lift: 4.700811850163794\n",
      "---\n",
      "Rule: ground beef -> herb & pepper\n",
      "Support: 0.015997866951073192\n",
      "Confidence: 0.3234501347708895\n",
      "Lift: 3.2919938411349285\n",
      "---\n",
      "Rule: ground beef -> tomato sauce\n",
      "Support: 0.005332622317024397\n",
      "Confidence: 0.3773584905660377\n",
      "Lift: 3.840659481324083\n",
      "---\n",
      "Rule: whole wheat pasta -> olive oil\n",
      "Support: 0.007998933475536596\n",
      "Confidence: 0.2714932126696833\n",
      "Lift: 4.122410097642296\n",
      "---\n",
      "Rule: shrimp -> pasta\n",
      "Support: 0.005065991201173177\n",
      "Confidence: 0.3220338983050847\n",
      "Lift: 4.506672147735896\n",
      "---\n",
      "Rule: chocolate -> shrimp\n",
      "Support: 0.005332622317024397\n",
      "Confidence: 0.23255813953488375\n",
      "Lift: 3.2545123221103784\n",
      "---\n",
      "Rule: ground beef -> spaghetti\n",
      "Support: 0.004799360085321957\n",
      "Confidence: 0.5714285714285714\n",
      "Lift: 3.2819951870487856\n",
      "---\n",
      "Rule: frozen vegetables -> spaghetti\n",
      "Support: 0.008665511265164644\n",
      "Confidence: 0.31100478468899523\n",
      "Lift: 3.165328208890303\n",
      "---\n",
      "Rule: frozen vegetables -> milk\n",
      "Support: 0.004799360085321957\n",
      "Confidence: 0.20338983050847456\n",
      "Lift: 3.088314005352364\n",
      "---\n",
      "Rule: mineral water -> shrimp\n",
      "Support: 0.007199040127982935\n",
      "Confidence: 0.30508474576271183\n",
      "Lift: 3.200616332819722\n",
      "---\n",
      "Rule: spaghetti -> frozen vegetables\n",
      "Support: 0.005732568990801226\n",
      "Confidence: 0.20574162679425836\n",
      "Lift: 3.1240241752707125\n",
      "---\n",
      "Rule: shrimp -> frozen vegetables\n",
      "Support: 0.005999200106652446\n",
      "Confidence: 0.21531100478468898\n",
      "Lift: 3.0131489680782684\n",
      "---\n",
      "Rule: tomatoes -> frozen vegetables\n",
      "Support: 0.006665777896280496\n",
      "Confidence: 0.23923444976076558\n",
      "Lift: 3.4980460188216425\n",
      "---\n",
      "Rule: ground beef -> grated cheese\n",
      "Support: 0.005332622317024397\n",
      "Confidence: 0.3225806451612903\n",
      "Lift: 3.283144395325426\n",
      "---\n",
      "Rule: mineral water -> ground beef\n",
      "Support: 0.006665777896280496\n",
      "Confidence: 0.39062500000000006\n",
      "Lift: 3.975682666214383\n",
      "---\n",
      "Rule: spaghetti -> ground beef\n",
      "Support: 0.006399146780429276\n",
      "Confidence: 0.3934426229508197\n",
      "Lift: 4.004359721511667\n",
      "---\n",
      "Rule: ground beef -> milk\n",
      "Support: 0.004932675643247567\n",
      "Confidence: 0.22424242424242427\n",
      "Lift: 3.40494417862839\n",
      "---\n",
      "Rule: shrimp -> ground beef\n",
      "Support: 0.005999200106652446\n",
      "Confidence: 0.5232558139534884\n",
      "Lift: 3.005315360233627\n",
      "---\n",
      "Rule: spaghetti -> milk\n",
      "Support: 0.007199040127982935\n",
      "Confidence: 0.20300751879699247\n",
      "Lift: 3.0825089038385434\n",
      "---\n",
      "Rule: mineral water -> olive oil\n",
      "Support: 0.005199306759098787\n",
      "Confidence: 0.22543352601156072\n",
      "Lift: 3.4230301186492245\n",
      "---\n",
      "Rule: spaghetti -> pancakes\n",
      "Support: 0.005065991201173177\n",
      "Confidence: 0.20105820105820105\n",
      "Lift: 3.0529100529100526\n",
      "---\n",
      "Rule: mineral water -> frozen vegetables\n",
      "Support: 0.004532728969470737\n",
      "Confidence: 0.28813559322033894\n",
      "Lift: 3.0228043143297376\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for rule in association_rules:\n",
    "    # Get items involved in the rule\n",
    "    items =  [x for x in rule.items]\n",
    "    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n",
    "    \n",
    "    # Print the support of the rule\n",
    "    print(\"Support:\", rule.support)\n",
    "    \n",
    "    # Print the confidence of the rule\n",
    "    print(\"Confidence:\", rule.ordered_statistics[0][2])\n",
    "    \n",
    "    # Print the lift of the rule\n",
    "    print(\"Lift:\", rule.ordered_statistics[0][3])\n",
    "    \n",
    "    # Print a separator\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the algorithm uses a lot of storage space, the original transaction list eaten up during an execution of the algorithm in the implementation used here, i.e., in the end the transaction list is empty and needs to be recreated for every further execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "\n",
    "# Convert items in the dataframe to string\n",
    "for row in store_data.values:\n",
    "    transactions.append(str(item) for item in row if str(item) != 'nan') # Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example:\n",
    "\n",
    "results = apriori(transactions, min_support=0.005, min_confidence=0.3, min_lift=4, min_length=2)\n",
    "\n",
    "association_rules = list(results) # Store rules in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rule in association_rules:\n",
    "    # Get items involved in the rule\n",
    "    items =  [x for x in rule.items]\n",
    "    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n",
    "    \n",
    "    # Print the support of the rule\n",
    "    print(\"Support:\", rule.support)\n",
    "    \n",
    "    # Print the confidence of the rule\n",
    "    print(\"Confidence:\", rule.ordered_statistics[0][2])\n",
    "    \n",
    "    # Print the lift of the rule\n",
    "    print(\"Lift:\", rule.ordered_statistics[0][3])\n",
    "    \n",
    "    # Print a separator\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 2. Using the Apriori implementation of mlxtend <a class=\"anchor\" id=\"mlxtend\"/>\n",
    "\n",
    "As an alternative to `apyori`, we use in the follwing code blocks the [mlxtend](https://rasbt.github.io/mlxtend/) (\"machine learning extensions\") library. Follow the instructions below to install mlxtend.\n",
    "\n",
    "**Installation**\n",
    "\n",
    "To install mlxtend, just execute on Terminal/CommandPrompt:\n",
    "\n",
    "```bash\n",
    "pip install mlxtend\n",
    "```\n",
    "\n",
    "If you use Anaconda, it is recommended to use the following command:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge mlxtend\n",
    "```\n",
    "\n",
    "**Don't forget to restart the kernel on this Jupyter Notebook**. Only after restarting the Jupyter kernel mlxtend can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With mlxtend installed, we can start by importing the appropriate functions and classes to run Apriori. Here we use `TransactionEncoder`, `apriori`, and `association_rules`. We also load the dataset using pandas and convert it to a list of transactions as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "store_data = pd.read_csv(os.path.join('Data', 'store_data.csv'), header=None)\n",
    "\n",
    "transactions = []\n",
    "\n",
    "# Convert items in the dataframe to string\n",
    "for row in store_data.values:\n",
    "    transactions.append([str(item) for item in row if str(item) != 'nan']) # Remove missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The `apriori` function expects data in a [one-hot encoded](https://en.wikipedia.org/wiki/One-hot?oldformat=true) Pandas dataframe. In a one-hot enconding, for each unique value in the original categorical column a new column is created. These dummy variables are then filled up with zeros and ones (1 meaning TRUE, 0 meaning FALSE). We can transform the dataset into the right format via the `TransactionEncoder` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's convert the enconded transactions into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "By default, `apriori` returns the column indices of the items. For better readability, we can set parameter `use_colnames=True` to convert these integer values into the respective item names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df, min_support=0.005, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With `frequent_itemset` we can generate association rules by calling the `association_rules` function. This function generates a dataframe including rules and metrics (i.e., score, confidence, and lift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "association_rules(frequent_itemsets, metric='lift', min_threshold=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leverage (A -> C): Leverage computes the difference between the observed frequency of A and C appearing together and the frequency that would be expected if A and C were independent. An leverage value of 0 indicates independence.\n",
    "\n",
    "Convinction(A->C): A high conviction value means that the consequent is highly depending on the antecedent. For instance, in the case of a perfect confidence score, the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as 'inf'. Similar to lift, if items are independent, the conviction is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 3. Shortcomings of Apriori <a class=\"anchor\" id=\"shortc\"/>\n",
    "\n",
    "The Apriori algorithm is very useful to find frequent itemsets in transaction data, but it comes with two major shortcomings. The first one refers to the often extremely large set of candidate itemsets generated during the search for frequent itemsets, which consumes a lot of memory. The second one refers to the time spend on counting the support of candidate itemsets since Apriori repeatedly scans the entire database.\n",
    "\n",
    "These shortcomings are a big problem when (1) we have a large volume of data to be analyzed, and (2) this analysis must be conducted on limited hardware (e.g., a Raspberry Pi).\n",
    "\n",
    "To overcome these problems, Computer Scientists Jiawei Han, Jian Pei, and Yiwen Yin developed the [FP-Growth](https://en.wikipedia.org/wiki/Association_rule_learning) algorithm in the early 2000s, for which we will look at an implementation next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 4. Frequent Pattern Growth <a class=\"anchor\" id=\"fpg\"/>\n",
    "\n",
    "The big advantage of FP-Growth in comparision to Apriori is that no candidate generation is required. Instead of performing an Apriori-like bottom-up generation of frequent itemset combinations, FP-Growth employs a divide-and-conquer method that only scans the database twice and uses a **frequent pattern tree** (FP-tree) to store data in a more compact way.\n",
    "\n",
    "The FP-tree is constructed by taking each itemset and mapping it to a path in the tree one at a time. The root starts with a null value, each node represents an item, while the association of the nodes is the itemsets with the order maintained while forming the tree. The whole idea behind this construction is that more frequently occurring itemsets will have a better chance of sharing items.\n",
    "\n",
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='img/fpgrowth.png' alt='Example of the FP-Growth algorithm' style=\"height:500px;\"/>\n",
    "    <span style='display:block;'>Example association rule mining using the FP-Growth algorithm. Image Source: <a href=\"http://qffc.uic.edu.hk/index.php/content/index/pid/276/cid/6533.html\" target=\"_blank\">Quantum Finance Forecast Center</a>.</span>\n",
    "    <br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Applying FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the following code blocks, we replace the apriori function with `fpgrowth`. Note that the entire procedure stays unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "store_data = pd.read_csv(os.path.join('Data', 'store_data.csv'), header=None)\n",
    "\n",
    "transactions = []\n",
    "\n",
    "# Convert items in the dataframe to string\n",
    "for row in store_data.values:\n",
    "    transactions.append([str(item) for item in row if str(item) != 'nan']) # Remove missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As in the case of `apriori`, the `fpgrowth` function expects data in a [one-hot encoded](https://en.wikipedia.org/wiki/One-hot?oldformat=true) Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We now convert the enconded transactions into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For better readability, we can set parameter `use_colnames=True` to convert these integer values into the respective item names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "frequent_itemsets = fpgrowth(df, min_support=0.005, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We call the `association_rules` function to generate a data frame of including rules and metrics (i.e., score, confidence, and lift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "association_rules(frequent_itemsets, metric='lift', min_threshold=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 5. An Alternative FP-Growth implementation <a class=\"anchor\" id=\"fpg2\"/>\n",
    "\n",
    "As an alternative to mlxtend, we use the [fpgrowth_py](https://github.com/chonyy/fpgrowth_py) package. To install it, you just need to run the command:\n",
    "\n",
    "```bash\n",
    "pip install fpgrowth_py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, let's use the fpgrowth_py to mine association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fpgrowth_py import fpgrowth\n",
    "\n",
    "store_data = pd.read_csv(os.path.join('Data', 'store_data.csv'), header=None)\n",
    "\n",
    "transactions = []\n",
    "\n",
    "for row in store_data.values:\n",
    "    transactions.append([str(item) for item in row if str(item) != 'nan']) # Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# minSupRatio is used to compute the support threshold:\n",
    "# minSup = len(itemSetList) * minSupRatio\n",
    "freqItemSet, rules = fpgrowth(transactions, minSupRatio=0.005, minConf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for rule in rules:\n",
    "    # Get items involved in the rule\n",
    "    print(\"Rule: \", rule[0], \"-> \", rule[1])\n",
    "    \n",
    "    # Print the support of the rule\n",
    "    print(\"Confidence:\", rule[2])\n",
    "    \n",
    "    # Print a separator\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### A quick comparison between FP-Growth and Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style='align: left; text-align:center;'>\n",
    "    <img src='img/comp_fpgrowth_apriori.png' alt='Comparison between FP-Growth and Apriori' style=\"height:300px;\"/>\n",
    "    <span style='display:block;'>A comparison between FP-Growth and Apriori. Image Source: <a href=\"https://towardsdatascience.com/fp-growth-frequent-pattern-generation-in-data-mining-with-python-implementation-244e561ab1c3\" target=\"_blank\">Towards Data Science</a>.</span>\n",
    "    <br/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 6. Exercise: The LastFM Dataset <a class=\"anchor\" id=\"lastfm\"/>\n",
    "\n",
    "To understand what exactly a listener prefers listening to on the radio, every detail is recorded online. This recorded information is used for recommending music that the listener is likely to enjoy and to come up with a focused marketing strategy that sends out advertisements for music that a listener may wish to buy. However, this results in wasting money on scarce advertising.\n",
    "\n",
    "Now consider the LastFM dataset extracted from Kaggle (and stored at the Data directory). Suppose that you are provided with data from a music community site, giving you details of each user. This will further help you get access to a log of every artist that the listed users have downloaded on their computer. The objective of providing this data consists of building a system that recommends new artists to the users in this listed community.\n",
    "\n",
    "How would you apply association rule mining in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End M2E05: Python Implementations of Association Rule Mining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
